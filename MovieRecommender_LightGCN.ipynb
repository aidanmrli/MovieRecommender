{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMiSjYEdz4O0gu6f0xF7ETC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aidanmrli/MovieRecommender/blob/main/MovieRecommender_LightGCN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "\n",
        "In this colab, we will learn about GNN pipelines by walking through the LightGCN model, and using it to recommend new movies to individual users through collaborative filtering.\n",
        "\n",
        "This MovieLens dataset consists of about 100,000 ratings applied to 9,000 movies by 600 users."
      ],
      "metadata": {
        "id": "Wj4VHwmcAS7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general, the GNN pipeline works as follows:\n",
        "\n",
        "1. Generate input graph\n",
        "2. Graph goes through GNN layers\n",
        "3. Set of node embeddings is produced as output from GNN \n",
        "4. Output of GNN is inputted to \"prediction head\" function to yield a prediction\n",
        "5. Predictions are compared against ground-truth labels\n",
        "6. Loss function & Evaluation"
      ],
      "metadata": {
        "id": "SofyYcJ3IozQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages.\n",
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install torch-geometric\n",
        "!pip install -q git+https://github.com/snap-stanford/deepsnap.git\n",
        "!pip install -U -q PyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SktcYOvrnRM",
        "outputId": "0b774c05-9e80-4774-9aa0-ba63c91bdf76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.11.0+cu113\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 33.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 38.2 MB/s \n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.0.4.tar.gz (407 kB)\n",
            "\u001b[K     |████████████████████████████████| 407 kB 20.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.0.4-py3-none-any.whl size=616603 sha256=f8a3a9e8122fc62d797d959926f3052430e7ceba76a466e886ecb5352607ce9e\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/a6/a4/ca18c3051fcead866fe7b85700ee2240d883562a1bc70ce421\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.0.4\n",
            "  Building wheel for deepsnap (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import required modules\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim, Tensor\n",
        "\n",
        "from torch_sparse import SparseTensor, matmul\n",
        "\n",
        "from torch_geometric.utils import structured_negative_sampling\n",
        "from torch_geometric.data import download_url, extract_zip\n",
        "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.typing import Adj"
      ],
      "metadata": {
        "id": "Axtw5ObR0tuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the Dataset (Preprocessing)\n",
        "\n",
        "We load the dataset and set ratings >=4 on a 0.5 ~ 5 scale as an edge between users and movies.\n",
        "\n",
        "We split the edges of the graph **once** using a 80/10/10 train/validation/test split. These sets of edges are disjoint sets.\n",
        "\n",
        "*   Training set used for optimizing GNN model  parameters\n",
        "*   Validation set used for tuning hyperparameters/develop model \n",
        "*   Once model is finalised, use test set to report final performance of model\n",
        "\n",
        "This is called a fixed split, but there are other strategies like random splitting where average performance over multiple seeds is measured."
      ],
      "metadata": {
        "id": "IIW2WQQk1JYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the dataset\n",
        "url = 'https://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
        "extract_zip(download_url(url, '.'), '.')\n",
        "\n",
        "movie_path = './ml-latest-small/movies.csv'\n",
        "rating_path = './ml-latest-small/ratings.csv'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9Mn6k811MJv",
        "outputId": "9b68bfce-e2e4-4aa0-fd08-b115e605cd05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
            "Extracting ./ml-latest-small.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load user and movie nodes\n",
        "def load_node_csv(path, index_col):\n",
        "    \"\"\"Loads csv containing node information\n",
        "\n",
        "    Args:\n",
        "        path (str): path to csv file\n",
        "        index_col (str): column name of index column\n",
        "\n",
        "    Returns:\n",
        "        dict: mapping of csv row to node id\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(path, index_col=index_col)\n",
        "    mapping = {index: i for i, index in enumerate(df.index.unique())}\n",
        "    return mapping\n",
        "\n",
        "\n",
        "user_mapping = load_node_csv(rating_path, index_col='userId')\n",
        "movie_mapping = load_node_csv(movie_path, index_col='movieId')"
      ],
      "metadata": {
        "id": "H3entpmg1Tqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the below block, we manipulate the graph structure by only adding edges with rating >= 4 out of 5 (originally on a 0.5–5 scale). \n",
        "\n",
        "*   Helps facilitate efficient message passing in the computation graph (not too many edges) \n",
        "*   Simplifies objective to edge/link prediction between nodes"
      ],
      "metadata": {
        "id": "b1ERzjda1xom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load edges between users and movies\n",
        "def load_edge_csv(path, src_index_col, src_mapping, dst_index_col, dst_mapping, link_index_col, rating_threshold=4):\n",
        "    \"\"\"Loads csv containing edges between users and items\n",
        "\n",
        "    Args:\n",
        "        path (str): path to csv file\n",
        "        src_index_col (str): column name of users\n",
        "        src_mapping (dict): mapping between row number and user id\n",
        "        dst_index_col (str): column name of items\n",
        "        dst_mapping (dict): mapping between row number and item id\n",
        "        link_index_col (str): column name of user item interaction\n",
        "        rating_threshold (int, optional): Threshold to determine positivity of edge. Defaults to 4.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: 2 by N matrix containing the node ids of N user-item edges\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(path)\n",
        "    edge_index = None\n",
        "    src = [src_mapping[index] for index in df[src_index_col]]\n",
        "    dst = [dst_mapping[index] for index in df[dst_index_col]]\n",
        "    edge_attr = torch.from_numpy(df[link_index_col].values).view(-1, 1).to(torch.long) >= rating_threshold\n",
        "\n",
        "\n",
        "    edge_index = [[], []]\n",
        "    for i in range(edge_attr.shape[0]):\n",
        "        if edge_attr[i]:\n",
        "            edge_index[0].append(src[i])\n",
        "            edge_index[1].append(dst[i])\n",
        "\n",
        "    return torch.tensor(edge_index)\n",
        "\n",
        "\n",
        "edge_index = load_edge_csv(\n",
        "    rating_path,\n",
        "    src_index_col='userId',\n",
        "    src_mapping=user_mapping,\n",
        "    dst_index_col='movieId',\n",
        "    dst_mapping=movie_mapping,\n",
        "    link_index_col='rating',\n",
        "    rating_threshold=4,\n",
        ")"
      ],
      "metadata": {
        "id": "vtgxMoaR1XDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notes about graph structure:\n",
        "\n",
        "We have a **homogeneous graph** (no distinction between the user and movie nodes in the graph structure). \n",
        "\n",
        "Graph is **bipartite**, with edges existing only between users and movies.\n",
        "\n",
        "The adjacency matrix of our homogeneous graph representation is sparse (most elements are 0) so we can represent our adjacency matrix with a PyG `SparseTensor` to reduce memory overheads."
      ],
      "metadata": {
        "id": "1WRkZ-xCNC-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MessagePassing interface of PyG relies on a gather-scatter scheme to aggregate messages from neighboring nodes. However, it has the disadvantage of explicitly materializing source & target node features, resulting in a high memory footprint on large and dense graphs.\n",
        "\n",
        "In some cases, like LightGCN, we can represent a GNN by a sparse-matrix multiplication, resulting in a lower memory footprint and a faster execution time. \n",
        "\n",
        "Sparse-matrix multiplication is suitable for GNNs if, in message computation, they:\n",
        "1. Do not use central node features `x_i`\n",
        "2. Do not use multi-dimensional edge features\n",
        "\n"
      ],
      "metadata": {
        "id": "98bJOpLs1kSd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting the graph\n",
        "\n",
        "Since we are working on an edge prediction task, we choose to transductively split our input graph by edges.\n",
        "\n",
        "Transductive splitting means the entire input graph is used in all of the training, validation, and test splits. We use the entire input graph to compute the embeddings in training, and only train using the labels of edges in the training set. Likewise, only the labels of validation edges are used to evaluate the validation set.\n",
        "\n",
        "This has the benefit of retaining all the information in the graph. However, there may be some data leakage between the sets.\n",
        "<!-- \n",
        "In theory, some of the training edges should be assigned as \"training supervision edges\", and hidden from the GNN in training. So the hierarchy of edges is as follows:\n",
        "\n",
        "1.   Training: Use training message edges to predict supervision edges\n",
        "2.   Validation: Use training message & supervision edges to predict validation edges\n",
        "3. Test: Use all non-test edges to predict test edges\n",
        " -->\n"
      ],
      "metadata": {
        "id": "ART7HZ4tOtWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split the edges of the graph using a 80/10/10 train/validation/test split\n",
        "num_users, num_movies = len(user_mapping), len(movie_mapping)\n",
        "num_interactions = edge_index.shape[1]\n",
        "all_indices = [i for i in range(num_interactions)]\n",
        "\n",
        "train_indices, test_indices = train_test_split(\n",
        "    all_indices, test_size=0.2, random_state=1)\n",
        "val_indices, test_indices = train_test_split(\n",
        "    test_indices, test_size=0.5, random_state=1)\n",
        "\n",
        "train_edge_index = edge_index[:, train_indices]\n",
        "val_edge_index = edge_index[:, val_indices]\n",
        "test_edge_index = edge_index[:, test_indices]"
      ],
      "metadata": {
        "id": "O3gzOpQk1iwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert edge indices into Sparse Tensors: https://pytorch-geometric.readthedocs.io/en/latest/notes/sparse_tensor.html\n",
        "train_sparse_edge_index = SparseTensor(row=train_edge_index[0], col=train_edge_index[1], sparse_sizes=(\n",
        "    num_users + num_movies, num_users + num_movies))\n",
        "val_sparse_edge_index = SparseTensor(row=val_edge_index[0], col=val_edge_index[1], sparse_sizes=(\n",
        "    num_users + num_movies, num_users + num_movies))\n",
        "test_sparse_edge_index = SparseTensor(row=test_edge_index[0], col=test_edge_index[1], sparse_sizes=(\n",
        "    num_users + num_movies, num_users + num_movies))"
      ],
      "metadata": {
        "id": "IxL0IY_h5T58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To leverage sparse-matrix multiplications, the MessagePassing interface introduces the message_and_aggregate() function (which fuses the message() and aggregate() functions into a single computation step), which gets called whenever it is implemented and receives a SparseTensor as input for edge_index."
      ],
      "metadata": {
        "id": "uxy7-KRd6sOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing LightGCN\n",
        "\n",
        "## Idea behind Light Graph Convolution Networks\n",
        "\n",
        "The message and aggregation functions of a specific GNN propagate information at each layer. \n",
        "LightGCN simplifies propagation by *removing* the non-linearity function, feature transformation matrices, and skip connection(s). \n",
        "\n",
        "The LightGCN authors argue feature transformation and nonlinear activation (designs common in GCN) contribute little to the performance of collaborative filtering. In fact, including them adds to the difficulty of training and may degrade recommendation performance. LightGCN includes only the most essential component in GCN -- neighborhood aggregation.\n",
        "\n",
        "For this use case, since the inputs to collaborative filtering are merely the node ids (without rich features), performing multiple non-linear transformations will not contribute to better performance.\n",
        "\n",
        "\n",
        "## Layer Combination\n",
        "The only trainable parameters of LightGCN are the 0-th layer embeddings $e_u^{(0)}$ and $e_i^{(0)}$ for each user and item. \n",
        "\n",
        "After k-levels of simple diffusion propagation, the final embedding of nodes will be a weighted average of the embeddings at every layer. This helps to approximate self-loops in nodes where the final embedding is informed by the embedding of the node itself and the neighbors and also helps to prevent over-smoothing.\n",
        "\n",
        "We combine the embeddings obtained at each layer of propagation to form the final embeddings for all user and item, $e_u$ and $e_i$ via the following equation.\n",
        "\n",
        "\\begin{equation}\n",
        "e_u = \\sum_{k = 0}^K \\alpha_k e_u^{(k)} \\quad e_i = \\sum_{k = 0}^K \\alpha_k e_i^{(k)}\n",
        "\\end{equation}\n",
        "\n",
        "$\\alpha_k$ : hyperparameter which weights the contribution of the k-th layer embedding to the final embedding\n",
        "\n",
        "## Model Prediction\n",
        "The model prediction (Prediction Head Function) is obtained by taking the inner product of the final embeddings of users and movies. \n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{y}_{ui} = e_u^Te_i\n",
        "\\end{equation}\n",
        "\n",
        "## Matrix Form\n",
        "This implementation uses the matrix form of LightGCN. We perform multi-scale diffusion to obtain the final embedding, which sums embeddings diffused across multi-hop scales. "
      ],
      "metadata": {
        "id": "ZLiDvu169k-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defines LightGCN model\n",
        "class LightGCN(MessagePassing):\n",
        "    \"\"\"LightGCN Model as proposed in https://arxiv.org/abs/2002.02126\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_users, num_items, embedding_dim=64, K=3, add_self_loops=False):\n",
        "        \"\"\"Initializes LightGCN Model\n",
        "\n",
        "        Args:\n",
        "            num_users (int): Number of users\n",
        "            num_items (int): Number of items\n",
        "            embedding_dim (int, optional): Dimensionality of embeddings. Defaults to 8.\n",
        "            K (int, optional): Number of message passing layers. Defaults to 3.\n",
        "            add_self_loops (bool, optional): Whether to add self loops for message passing. Defaults to False.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_users, self.num_items = num_users, num_items\n",
        "        self.embedding_dim, self.K = embedding_dim, K\n",
        "        self.add_self_loops = add_self_loops\n",
        "\n",
        "        self.users_emb = nn.Embedding(\n",
        "            num_embeddings=self.num_users, embedding_dim=self.embedding_dim) # e_u^0\n",
        "        self.items_emb = nn.Embedding(\n",
        "            num_embeddings=self.num_items, embedding_dim=self.embedding_dim) # e_i^0\n",
        "\n",
        "        nn.init.normal_(self.users_emb.weight, std=0.1)\n",
        "        nn.init.normal_(self.items_emb.weight, std=0.1)\n",
        "\n",
        "    def forward(self, edge_index: SparseTensor):\n",
        "        \"\"\"Forward propagation of LightGCN Model.\n",
        "\n",
        "        Args:\n",
        "            edge_index (SparseTensor): adjacency matrix\n",
        "\n",
        "        Returns:\n",
        "            tuple (Tensor): e_u_k, e_u_0, e_i_k, e_i_0\n",
        "        \"\"\"\n",
        "        # compute \\tilde{A}: symmetrically normalized adjacency matrix\n",
        "        edge_index_norm = gcn_norm(\n",
        "            edge_index, add_self_loops=self.add_self_loops)\n",
        "\n",
        "        emb_0 = torch.cat([self.users_emb.weight, self.items_emb.weight]) # E^0\n",
        "        embs = [emb_0]\n",
        "        emb_k = emb_0\n",
        "\n",
        "        # multi-scale diffusion\n",
        "        for i in range(self.K):\n",
        "            emb_k = self.propagate(edge_index_norm, x=emb_k)\n",
        "            embs.append(emb_k)\n",
        "\n",
        "        embs = torch.stack(embs, dim=1)\n",
        "        emb_final = torch.mean(embs, dim=1) # E^K\n",
        "\n",
        "        users_emb_final, items_emb_final = torch.split(\n",
        "            emb_final, [self.num_users, self.num_items]) # splits into e_u^K and e_i^K\n",
        "\n",
        "        # returns e_u^K, e_u^0, e_i^K, e_i^0\n",
        "        return users_emb_final, self.users_emb.weight, items_emb_final, self.items_emb.weight\n",
        "\n",
        "    def message(self, x_j: Tensor) -> Tensor:\n",
        "        return x_j\n",
        "\n",
        "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
        "        # computes \\tilde{A} @ x\n",
        "        return matmul(adj_t, x)\n",
        "\n",
        "model = LightGCN(num_users, num_movies)"
      ],
      "metadata": {
        "id": "JUEQ2nKOXyC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Function\n",
        "\n",
        "A loss function is used to measure the discrepancy between the model's predictions and the ground-truth labels. From there, we backpropagate to the parameters of the neural network, fine-tuning it to optimise the loss.\n",
        "\n",
        "We utilize a Bayesian Personalized Ranking (BPR) loss, a pairwise objective which encourages the predictions of positive samples to be higher than negative samples for each user.\n",
        "\n",
        "We use PyG’s `structured_negative_sampling` function to conduct negative sampling which we use to compute BPR loss along with the positive samples."
      ],
      "metadata": {
        "id": "fIY2_4poYHpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function which random samples a mini-batch of positive and negative samples\n",
        "def sample_mini_batch(batch_size, edge_index):\n",
        "    \"\"\"Randomly samples indices of a minibatch given an adjacency matrix\n",
        "\n",
        "    Args:\n",
        "        batch_size (int): minibatch size\n",
        "        edge_index (torch.Tensor): 2 by N list of edges\n",
        "\n",
        "    Returns:\n",
        "        tuple: user indices, positive item indices, negative item indices\n",
        "    \"\"\"\n",
        "    edges = structured_negative_sampling(edge_index)\n",
        "    edges = torch.stack(edges, dim=0)\n",
        "    indices = random.choices(\n",
        "        [i for i in range(edges[0].shape[0])], k=batch_size)\n",
        "    batch = edges[:, indices]\n",
        "    user_indices, pos_item_indices, neg_item_indices = batch[0], batch[1], batch[2]\n",
        "    return user_indices, pos_item_indices, neg_item_indices"
      ],
      "metadata": {
        "id": "zXZ_AwC05J7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bpr_loss(users_emb_final, users_emb_0, pos_items_emb_final, pos_items_emb_0, neg_items_emb_final, neg_items_emb_0, lambda_val):\n",
        "    \"\"\"Bayesian Personalized Ranking Loss as described in https://arxiv.org/abs/1205.2618\n",
        "\n",
        "    Args:\n",
        "        users_emb_final (torch.Tensor): e_u_k\n",
        "        users_emb_0 (torch.Tensor): e_u_0\n",
        "        pos_items_emb_final (torch.Tensor): positive e_i_k\n",
        "        pos_items_emb_0 (torch.Tensor): positive e_i_0\n",
        "        neg_items_emb_final (torch.Tensor): negative e_i_k\n",
        "        neg_items_emb_0 (torch.Tensor): negative e_i_0\n",
        "        lambda_val (float): lambda value for regularization loss term\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: scalar bpr loss value\n",
        "    \"\"\"\n",
        "    reg_loss = lambda_val * (users_emb_0.norm(2).pow(2) +\n",
        "                             pos_items_emb_0.norm(2).pow(2) +\n",
        "                             neg_items_emb_0.norm(2).pow(2)) # L2 loss\n",
        "\n",
        "    pos_scores = torch.mul(users_emb_final, pos_items_emb_final)\n",
        "    pos_scores = torch.sum(pos_scores, dim=-1) # predicted scores of positive samples\n",
        "    neg_scores = torch.mul(users_emb_final, neg_items_emb_final)\n",
        "    neg_scores = torch.sum(neg_scores, dim=-1) # predicted scores of negative samples\n",
        "\n",
        "    loss = -torch.mean(torch.nn.functional.softplus(pos_scores - neg_scores)) + reg_loss\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "W4i02LLbYI9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function to get N_u\n",
        "def get_user_positive_items(edge_index):\n",
        "    \"\"\"Generates dictionary of positive items for each user\n",
        "\n",
        "    Args:\n",
        "        edge_index (torch.Tensor): 2 by N list of edges\n",
        "\n",
        "    Returns:\n",
        "        dict: dictionary of positive items for each user\n",
        "    \"\"\"\n",
        "    user_pos_items = {}\n",
        "    for i in range(edge_index.shape[1]):\n",
        "        user = edge_index[0][i].item()\n",
        "        item = edge_index[1][i].item()\n",
        "        if user not in user_pos_items:\n",
        "            user_pos_items[user] = []\n",
        "        user_pos_items[user].append(item)\n",
        "    return user_pos_items"
      ],
      "metadata": {
        "id": "ypsGt82mYM63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "\n",
        "We will use a few metrics to evaluate the model:\n",
        "\n",
        "\n",
        "*   Recall: $\\frac{TP}{TP + FN}$\n",
        "\n",
        "*   Precision: $\\frac{TP}{TP + FP}$\n",
        "\n",
        "* Normalized Discounted Cumulative Gain (NDCG): takes into account the ordered ranking of recommendations. Recall and Precision are order invariant.\n"
      ],
      "metadata": {
        "id": "5QzVHO5GV5rO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# computes recall@K and precision@K\n",
        "def RecallPrecision_ATk(groundTruth, r, k):\n",
        "    \"\"\"Computers recall @ k and precision @ k\n",
        "\n",
        "    Args:\n",
        "        groundTruth (list): list of lists containing highly rated items of each user\n",
        "        r (list): list of lists indicating whether each top k item recommended to each user\n",
        "            is a top k ground truth item or not\n",
        "        k (intg): determines the top k items to compute precision and recall on\n",
        "\n",
        "    Returns:\n",
        "        tuple: recall @ k, precision @ k\n",
        "    \"\"\"\n",
        "    num_correct_pred = torch.sum(r, dim=-1)  # number of correctly predicted items per user\n",
        "    # number of items liked by each user in the test set\n",
        "    user_num_liked = torch.Tensor([len(groundTruth[i])\n",
        "                                  for i in range(len(groundTruth))])\n",
        "    recall = torch.mean(num_correct_pred / user_num_liked)\n",
        "    precision = torch.mean(num_correct_pred) / k\n",
        "    return recall.item(), precision.item()"
      ],
      "metadata": {
        "id": "tXW3z7zZYPCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# computes NDCG@K\n",
        "def NDCGatK_r(groundTruth, r, k):\n",
        "    \"\"\"Computes Normalized Discounted Cumulative Gain (NDCG) @ k\n",
        "\n",
        "    Args:\n",
        "        groundTruth (list): list of lists containing highly rated items of each user\n",
        "        r (list): list of lists indicating whether each top k item recommended to each user\n",
        "            is a top k ground truth item or not\n",
        "        k (int): determines the top k items to compute ndcg on\n",
        "\n",
        "    Returns:\n",
        "        float: ndcg @ k\n",
        "    \"\"\"\n",
        "    assert len(r) == len(groundTruth)\n",
        "\n",
        "    test_matrix = torch.zeros((len(r), k))\n",
        "\n",
        "    for i, items in enumerate(groundTruth):\n",
        "        length = min(len(items), k)\n",
        "        test_matrix[i, :length] = 1\n",
        "    max_r = test_matrix\n",
        "    idcg = torch.sum(max_r * 1. / torch.log2(torch.arange(2, k + 2)), axis=1)\n",
        "    dcg = r * (1. / torch.log2(torch.arange(2, k + 2)))\n",
        "    dcg = torch.sum(dcg, axis=1)\n",
        "    idcg[idcg == 0.] = 1.\n",
        "    ndcg = dcg / idcg\n",
        "    ndcg[torch.isnan(ndcg)] = 0.\n",
        "    return torch.mean(ndcg).item()"
      ],
      "metadata": {
        "id": "CbpFWjdTYRgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wrapper function to get evaluation metrics\n",
        "def get_metrics(model, edge_index, exclude_edge_indices, k):\n",
        "    \"\"\"Computes the evaluation metrics: recall, precision, and ndcg @ k\n",
        "\n",
        "    Args:\n",
        "        model (LighGCN): lightgcn model\n",
        "        edge_index (torch.Tensor): 2 by N list of edges for split to evaluate\n",
        "        exclude_edge_indices ([type]): 2 by N list of edges for split to discount from evaluation\n",
        "        k (int): determines the top k items to compute metrics on\n",
        "\n",
        "    Returns:\n",
        "        tuple: recall @ k, precision @ k, ndcg @ k\n",
        "    \"\"\"\n",
        "    user_embedding = model.users_emb.weight\n",
        "    item_embedding = model.items_emb.weight\n",
        "\n",
        "    # get ratings between every user and item - shape is num users x num movies\n",
        "    rating = torch.matmul(user_embedding, item_embedding.T)\n",
        "\n",
        "    for exclude_edge_index in exclude_edge_indices:\n",
        "        # gets all the positive items for each user from the edge index\n",
        "        user_pos_items = get_user_positive_items(exclude_edge_index)\n",
        "        # get coordinates of all edges to exclude\n",
        "        exclude_users = []\n",
        "        exclude_items = []\n",
        "        for user, items in user_pos_items.items():\n",
        "            exclude_users.extend([user] * len(items))\n",
        "            exclude_items.extend(items)\n",
        "\n",
        "        # set ratings of excluded edges to large negative value\n",
        "        rating[exclude_users, exclude_items] = -(1 << 10)\n",
        "\n",
        "    # get the top k recommended items for each user\n",
        "    _, top_K_items = torch.topk(rating, k=k)\n",
        "\n",
        "    # get all unique users in evaluated split\n",
        "    users = edge_index[0].unique()\n",
        "\n",
        "    test_user_pos_items = get_user_positive_items(edge_index)\n",
        "\n",
        "    # convert test user pos items dictionary into a list\n",
        "    test_user_pos_items_list = [\n",
        "        test_user_pos_items[user.item()] for user in users]\n",
        "\n",
        "    # determine the correctness of topk predictions\n",
        "    r = []\n",
        "    for user in users:\n",
        "        ground_truth_items = test_user_pos_items[user.item()]\n",
        "        label = list(map(lambda x: x in ground_truth_items, top_K_items[user]))\n",
        "        r.append(label)\n",
        "    r = torch.Tensor(np.array(r).astype('float'))\n",
        "\n",
        "    recall, precision = RecallPrecision_ATk(test_user_pos_items_list, r, k)\n",
        "    ndcg = NDCGatK_r(test_user_pos_items_list, r, k)\n",
        "\n",
        "    return recall, precision, ndcg"
      ],
      "metadata": {
        "id": "HaXMnaszYT3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wrapper function to evaluate model\n",
        "def evaluation(model, edge_index, sparse_edge_index, exclude_edge_indices, k, lambda_val):\n",
        "    \"\"\"Evaluates model loss and metrics including recall, precision, ndcg @ k\n",
        "\n",
        "    Args:\n",
        "        model (LightGCN): lightgcn model\n",
        "        edge_index (torch.Tensor): 2 by N list of edges for split to evaluate\n",
        "        sparse_edge_index (sparseTensor): sparse adjacency matrix for split to evaluate\n",
        "        exclude_edge_indices ([type]): 2 by N list of edges for split to discount from evaluation\n",
        "        k (int): determines the top k items to compute metrics on\n",
        "        lambda_val (float): determines lambda for bpr loss\n",
        "\n",
        "    Returns:\n",
        "        tuple: bpr loss, recall @ k, precision @ k, ndcg @ k\n",
        "    \"\"\"\n",
        "    # get embeddings\n",
        "    users_emb_final, users_emb_0, items_emb_final, items_emb_0 = model.forward(\n",
        "        sparse_edge_index)\n",
        "    edges = structured_negative_sampling(\n",
        "        edge_index, contains_neg_self_loops=False)\n",
        "    user_indices, pos_item_indices, neg_item_indices = edges[0], edges[1], edges[2]\n",
        "    users_emb_final, users_emb_0 = users_emb_final[user_indices], users_emb_0[user_indices]\n",
        "    pos_items_emb_final, pos_items_emb_0 = items_emb_final[\n",
        "        pos_item_indices], items_emb_0[pos_item_indices]\n",
        "    neg_items_emb_final, neg_items_emb_0 = items_emb_final[\n",
        "        neg_item_indices], items_emb_0[neg_item_indices]\n",
        "\n",
        "    loss = bpr_loss(users_emb_final, users_emb_0, pos_items_emb_final, pos_items_emb_0,\n",
        "                    neg_items_emb_final, neg_items_emb_0, lambda_val).item()\n",
        "\n",
        "    recall, precision, ndcg = get_metrics(\n",
        "        model, edge_index, exclude_edge_indices, k)\n",
        "\n",
        "    return loss, recall, precision, ndcg"
      ],
      "metadata": {
        "id": "BaVxUsnZYWNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n",
        "Your test set performance should be in line with the following (*K=20*):\n",
        "\n",
        "*Recall@K: 0.13, Precision@K: 0.045, NDCG@K: 0.10*"
      ],
      "metadata": {
        "id": "6gPt_ILUYsWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define contants\n",
        "ITERATIONS = 2500\n",
        "BATCH_SIZE = 1024\n",
        "LR = 1e-3\n",
        "ITERS_PER_EVAL = 200\n",
        "ITERS_PER_LR_DECAY = 200\n",
        "K = 20\n",
        "LAMBDA = 1e-6"
      ],
      "metadata": {
        "id": "mBUpE650YsEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device {device}.\")\n",
        "\n",
        "\n",
        "model = model.to(device)\n",
        "model.train()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
        "\n",
        "edge_index = edge_index.to(device)\n",
        "train_edge_index = train_edge_index.to(device)\n",
        "train_sparse_edge_index = train_sparse_edge_index.to(device)\n",
        "\n",
        "val_edge_index = val_edge_index.to(device)\n",
        "val_sparse_edge_index = val_sparse_edge_index.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbYbadsyYwxY",
        "outputId": "69fab729-6818-406c-daeb-d99faac70d42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device cuda.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for iter in range(ITERATIONS):\n",
        "    # forward propagation\n",
        "    users_emb_final, users_emb_0, items_emb_final, items_emb_0 = model.forward(\n",
        "        train_sparse_edge_index)\n",
        "\n",
        "    # mini batching\n",
        "    user_indices, pos_item_indices, neg_item_indices = sample_mini_batch(\n",
        "        BATCH_SIZE, train_edge_index)\n",
        "    user_indices, pos_item_indices, neg_item_indices = user_indices.to(\n",
        "        device), pos_item_indices.to(device), neg_item_indices.to(device)\n",
        "    users_emb_final, users_emb_0 = users_emb_final[user_indices], users_emb_0[user_indices]\n",
        "    pos_items_emb_final, pos_items_emb_0 = items_emb_final[\n",
        "        pos_item_indices], items_emb_0[pos_item_indices]\n",
        "    neg_items_emb_final, neg_items_emb_0 = items_emb_final[\n",
        "        neg_item_indices], items_emb_0[neg_item_indices]\n",
        "\n",
        "    # loss computation\n",
        "    train_loss = bpr_loss(users_emb_final, users_emb_0, pos_items_emb_final,\n",
        "                          pos_items_emb_0, neg_items_emb_final, neg_items_emb_0, LAMBDA)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if iter % ITERS_PER_EVAL == 0:\n",
        "        model.eval()\n",
        "        val_loss, recall, precision, ndcg = evaluation(\n",
        "            model, val_edge_index, val_sparse_edge_index, [train_edge_index], K, LAMBDA)\n",
        "        print(f\"[Iteration {iter}/{ITERATIONS}] train_loss: {round(train_loss.item(), 5)}, val_loss: {round(val_loss, 5)}, val_recall@{K}: {round(recall, 5)}, val_precision@{K}: {round(precision, 5)}, val_ndcg@{K}: {round(ndcg, 5)}\")\n",
        "        train_losses.append(train_loss.item())\n",
        "        val_losses.append(val_loss)\n",
        "        model.train()\n",
        "\n",
        "    if iter % ITERS_PER_LR_DECAY == 0 and iter != 0:\n",
        "        scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "EiOxtTDIYzkw",
        "outputId": "fe57b509-64f4-4ec3-c658-a8dc7684542e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Iteration 0/2500] train_loss: -75.71837, val_loss: -63.64639, val_recall@20: 0.15277, val_precision@20: 0.04629, val_ndcg@20: 0.10708\n",
            "[Iteration 200/2500] train_loss: -82.37093, val_loss: -67.82334, val_recall@20: 0.15449, val_precision@20: 0.04629, val_ndcg@20: 0.10748\n",
            "[Iteration 400/2500] train_loss: -89.46757, val_loss: -73.83046, val_recall@20: 0.15479, val_precision@20: 0.04647, val_ndcg@20: 0.1075\n",
            "[Iteration 600/2500] train_loss: -96.74461, val_loss: -78.09077, val_recall@20: 0.15274, val_precision@20: 0.0462, val_ndcg@20: 0.10707\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-e9e35014a678>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# mini batching\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     user_indices, pos_item_indices, neg_item_indices = sample_mini_batch(\n\u001b[0;32m---> 12\u001b[0;31m         BATCH_SIZE, train_edge_index)\n\u001b[0m\u001b[1;32m     13\u001b[0m     user_indices, pos_item_indices, neg_item_indices = user_indices.to(\n\u001b[1;32m     14\u001b[0m         device), pos_item_indices.to(device), neg_item_indices.to(device)\n",
            "\u001b[0;32m<ipython-input-10-1c60ff43802c>\u001b[0m in \u001b[0;36msample_mini_batch\u001b[0;34m(batch_size, edge_index)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0muser\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive\u001b[0m \u001b[0mitem\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m \u001b[0mitem\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \"\"\"\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0medges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructured_negative_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0medges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     indices = random.choices(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_geometric/utils/negative_sampling.py\u001b[0m in \u001b[0;36mstructured_negative_sampling\u001b[0;34m(edge_index, num_nodes, contains_neg_self_loops)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mneg_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrest\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_nodes\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m         \u001b[0mrest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36misin\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36misin\u001b[0;34m(element, test_elements, assume_unique, invert)\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m     return in1d(element, test_elements, assume_unique=assume_unique,\n\u001b[0;32m--> 736\u001b[0;31m                 invert=invert).reshape(element.shape)\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36min1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36min1d\u001b[0;34m(ar1, ar2, assume_unique, invert)\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0massume_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0mar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrev_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0mar2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mar2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unpack_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate on test set\n",
        "model.eval()\n",
        "test_edge_index = test_edge_index.to(device)\n",
        "test_sparse_edge_index = test_sparse_edge_index.to(device)\n",
        "\n",
        "test_loss, test_recall, test_precision, test_ndcg = evaluation(\n",
        "            model, test_edge_index, test_sparse_edge_index, [train_edge_index, val_edge_index], K, LAMBDA)\n",
        "\n",
        "print(f\"[test_loss: {round(test_loss, 5)}, test_recall@{K}: {round(test_recall, 5)}, test_precision@{K}: {round(test_precision, 5)}, test_ndcg@{K}: {round(test_ndcg, 5)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ii3L1niY3Da",
        "outputId": "b3cf36e2-1c9e-414e-ede7-78ea2637eb18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[test_loss: -72.16206, test_recall@20: 0.12687, test_precision@20: 0.04683, test_ndcg@20: 0.1014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make new recommendations for a given user!"
      ],
      "metadata": {
        "id": "YM1wXxrXY5Uj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "df = pd.read_csv(movie_path)\n",
        "movieid_title = pd.Series(df.title.values,index=df.movieId).to_dict()\n",
        "movieid_genres = pd.Series(df.genres.values,index=df.movieId).to_dict()\n",
        "\n",
        "user_pos_items = get_user_positive_items(edge_index)"
      ],
      "metadata": {
        "id": "lg5A0GqGY74p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_predictions(user_id, num_recs):\n",
        "    user = user_mapping[user_id]\n",
        "    e_u = model.users_emb.weight[user]\n",
        "    scores = model.items_emb.weight @ e_u\n",
        "\n",
        "    values, indices = torch.topk(scores, k=len(user_pos_items[user]) + num_recs)\n",
        "\n",
        "    movies = [index.cpu().item() for index in indices if index in user_pos_items[user]][:num_recs]\n",
        "    movie_ids = [list(movie_mapping.keys())[list(movie_mapping.values()).index(movie)] for movie in movies]\n",
        "    titles = [movieid_title[id] for id in movie_ids]\n",
        "    genres = [movieid_genres[id] for id in movie_ids]\n",
        "\n",
        "    print(f\"Here are some movies that user {user_id} rated highly\")\n",
        "    for i in range(num_recs):\n",
        "        print(f\"title: {titles[i]}, genres: {genres[i]} \")\n",
        "\n",
        "    print()\n",
        "\n",
        "    movies = [index.cpu().item() for index in indices if index not in user_pos_items[user]][:num_recs]\n",
        "    movie_ids = [list(movie_mapping.keys())[list(movie_mapping.values()).index(movie)] for movie in movies]\n",
        "    titles = [movieid_title[id] for id in movie_ids]\n",
        "    genres = [movieid_genres[id] for id in movie_ids]\n",
        "\n",
        "    print(f\"Here are some suggested movies for user {user_id}\")\n",
        "    for i in range(num_recs):\n",
        "        print(f\"title: {titles[i]}, genres: {genres[i]} \")"
      ],
      "metadata": {
        "id": "QrF4RQlbY-dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_predictions(user_id=420, num_recs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqvIJTN2fct8",
        "outputId": "633e20f2-e126-4d5e-c0f1-cd73848d1389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are some movies that user 420 rated highly\n",
            "title: Shawshank Redemption, The (1994), genres: Crime|Drama \n",
            "title: Forrest Gump (1994), genres: Comedy|Drama|Romance|War \n",
            "title: Pulp Fiction (1994), genres: Comedy|Crime|Drama|Thriller \n",
            "title: Matrix, The (1999), genres: Action|Sci-Fi|Thriller \n",
            "title: Fight Club (1999), genres: Action|Crime|Drama|Thriller \n",
            "title: Schindler's List (1993), genres: Drama|War \n",
            "title: Toy Story (1995), genres: Adventure|Animation|Children|Comedy|Fantasy \n",
            "title: Lord of the Rings: The Fellowship of the Ring, The (2001), genres: Adventure|Fantasy \n",
            "title: American Beauty (1999), genres: Drama|Romance \n",
            "title: Lord of the Rings: The Return of the King, The (2003), genres: Action|Adventure|Drama|Fantasy \n",
            "title: Lord of the Rings: The Two Towers, The (2002), genres: Adventure|Fantasy \n",
            "title: Aladdin (1992), genres: Adventure|Animation|Children|Comedy|Musical \n",
            "title: Sixth Sense, The (1999), genres: Drama|Horror|Mystery \n",
            "title: Lion King, The (1994), genres: Adventure|Animation|Children|Drama|Musical|IMAX \n",
            "title: Eternal Sunshine of the Spotless Mind (2004), genres: Drama|Romance|Sci-Fi \n",
            "title: One Flew Over the Cuckoo's Nest (1975), genres: Drama \n",
            "title: Shrek (2001), genres: Adventure|Animation|Children|Comedy|Fantasy|Romance \n",
            "title: American History X (1998), genres: Crime|Drama \n",
            "title: Monty Python and the Holy Grail (1975), genres: Adventure|Comedy|Fantasy \n",
            "title: Kill Bill: Vol. 1 (2003), genres: Action|Crime|Thriller \n",
            "\n",
            "Here are some suggested movies for user 420\n",
            "title: Silence of the Lambs, The (1991), genres: Crime|Horror|Thriller \n",
            "title: Star Wars: Episode IV - A New Hope (1977), genres: Action|Adventure|Sci-Fi \n",
            "title: Usual Suspects, The (1995), genres: Crime|Mystery|Thriller \n",
            "title: Star Wars: Episode V - The Empire Strikes Back (1980), genres: Action|Adventure|Sci-Fi \n",
            "title: Braveheart (1995), genres: Action|Drama|War \n",
            "title: Godfather, The (1972), genres: Crime|Drama \n",
            "title: Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981), genres: Action|Adventure \n",
            "title: Star Wars: Episode VI - Return of the Jedi (1983), genres: Action|Adventure|Sci-Fi \n",
            "title: Fugitive, The (1993), genres: Thriller \n",
            "title: Jurassic Park (1993), genres: Action|Adventure|Sci-Fi|Thriller \n",
            "title: Terminator 2: Judgment Day (1991), genres: Action|Sci-Fi \n",
            "title: Apollo 13 (1995), genres: Adventure|Drama|IMAX \n",
            "title: Seven (a.k.a. Se7en) (1995), genres: Mystery|Thriller \n",
            "title: Saving Private Ryan (1998), genres: Action|Drama|War \n",
            "title: Fargo (1996), genres: Comedy|Crime|Drama|Thriller \n",
            "title: Back to the Future (1985), genres: Adventure|Comedy|Sci-Fi \n",
            "title: Godfather: Part II, The (1974), genres: Crime|Drama \n",
            "title: Princess Bride, The (1987), genres: Action|Adventure|Comedy|Fantasy|Romance \n",
            "title: Twelve Monkeys (a.k.a. 12 Monkeys) (1995), genres: Mystery|Sci-Fi|Thriller \n",
            "title: Indiana Jones and the Last Crusade (1989), genres: Action|Adventure \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "make_predictions(user_id=1, num_recs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJTtpyErZBIN",
        "outputId": "31c11fb3-6812-4b77-b9dd-9ccac0b074cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are some movies that user 1 rated highly\n",
            "title: Forrest Gump (1994), genres: Comedy|Drama|Romance|War \n",
            "title: Silence of the Lambs, The (1991), genres: Crime|Horror|Thriller \n",
            "title: Matrix, The (1999), genres: Action|Sci-Fi|Thriller \n",
            "title: Star Wars: Episode IV - A New Hope (1977), genres: Action|Adventure|Sci-Fi \n",
            "title: Fight Club (1999), genres: Action|Crime|Drama|Thriller \n",
            "title: Usual Suspects, The (1995), genres: Crime|Mystery|Thriller \n",
            "title: Schindler's List (1993), genres: Drama|War \n",
            "title: Star Wars: Episode V - The Empire Strikes Back (1980), genres: Action|Adventure|Sci-Fi \n",
            "title: Braveheart (1995), genres: Action|Drama|War \n",
            "title: Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981), genres: Action|Adventure \n",
            "title: Star Wars: Episode VI - Return of the Jedi (1983), genres: Action|Adventure|Sci-Fi \n",
            "title: Toy Story (1995), genres: Adventure|Animation|Children|Comedy|Fantasy \n",
            "title: Fugitive, The (1993), genres: Thriller \n",
            "title: Jurassic Park (1993), genres: Action|Adventure|Sci-Fi|Thriller \n",
            "title: American Beauty (1999), genres: Drama|Romance \n",
            "title: Seven (a.k.a. Se7en) (1995), genres: Mystery|Thriller \n",
            "title: Saving Private Ryan (1998), genres: Action|Drama|War \n",
            "title: Fargo (1996), genres: Comedy|Crime|Drama|Thriller \n",
            "title: Back to the Future (1985), genres: Adventure|Comedy|Sci-Fi \n",
            "title: Princess Bride, The (1987), genres: Action|Adventure|Comedy|Fantasy|Romance \n",
            "\n",
            "Here are some suggested movies for user 1\n",
            "title: Shawshank Redemption, The (1994), genres: Crime|Drama \n",
            "title: Pulp Fiction (1994), genres: Comedy|Crime|Drama|Thriller \n",
            "title: Godfather, The (1972), genres: Crime|Drama \n",
            "title: Lord of the Rings: The Fellowship of the Ring, The (2001), genres: Adventure|Fantasy \n",
            "title: Terminator 2: Judgment Day (1991), genres: Action|Sci-Fi \n",
            "title: Apollo 13 (1995), genres: Adventure|Drama|IMAX \n",
            "title: Lord of the Rings: The Return of the King, The (2003), genres: Action|Adventure|Drama|Fantasy \n",
            "title: Lord of the Rings: The Two Towers, The (2002), genres: Adventure|Fantasy \n",
            "title: Aladdin (1992), genres: Adventure|Animation|Children|Comedy|Musical \n",
            "title: Sixth Sense, The (1999), genres: Drama|Horror|Mystery \n",
            "title: Lion King, The (1994), genres: Adventure|Animation|Children|Drama|Musical|IMAX \n",
            "title: Eternal Sunshine of the Spotless Mind (2004), genres: Drama|Romance|Sci-Fi \n",
            "title: Godfather: Part II, The (1974), genres: Crime|Drama \n",
            "title: Twelve Monkeys (a.k.a. 12 Monkeys) (1995), genres: Mystery|Sci-Fi|Thriller \n",
            "title: One Flew Over the Cuckoo's Nest (1975), genres: Drama \n",
            "title: Die Hard (1988), genres: Action|Crime|Thriller \n",
            "title: Good Will Hunting (1997), genres: Drama|Romance \n",
            "title: Memento (2000), genres: Mystery|Thriller \n",
            "title: Shrek (2001), genres: Adventure|Animation|Children|Comedy|Fantasy|Romance \n",
            "title: Finding Nemo (2003), genres: Adventure|Animation|Children|Comedy \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "make_predictions(user_id=95, num_recs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAp7fqb7dfe7",
        "outputId": "4b73f5f3-32bf-491b-f929-d9cfd583b851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are some movies that user 95 rated highly\n",
            "title: Matrix, The (1999), genres: Action|Sci-Fi|Thriller \n",
            "title: Star Wars: Episode IV - A New Hope (1977), genres: Action|Adventure|Sci-Fi \n",
            "title: Star Wars: Episode V - The Empire Strikes Back (1980), genres: Action|Adventure|Sci-Fi \n",
            "title: Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981), genres: Action|Adventure \n",
            "title: Star Wars: Episode VI - Return of the Jedi (1983), genres: Action|Adventure|Sci-Fi \n",
            "title: Terminator 2: Judgment Day (1991), genres: Action|Sci-Fi \n",
            "title: Saving Private Ryan (1998), genres: Action|Drama|War \n",
            "title: Fargo (1996), genres: Comedy|Crime|Drama|Thriller \n",
            "title: Lord of the Rings: The Two Towers, The (2002), genres: Adventure|Fantasy \n",
            "title: Back to the Future (1985), genres: Adventure|Comedy|Sci-Fi \n",
            "title: Eternal Sunshine of the Spotless Mind (2004), genres: Drama|Romance|Sci-Fi \n",
            "title: Godfather: Part II, The (1974), genres: Crime|Drama \n",
            "title: Twelve Monkeys (a.k.a. 12 Monkeys) (1995), genres: Mystery|Sci-Fi|Thriller \n",
            "title: Die Hard (1988), genres: Action|Crime|Thriller \n",
            "title: Alien (1979), genres: Horror|Sci-Fi \n",
            "title: Reservoir Dogs (1992), genres: Crime|Mystery|Thriller \n",
            "title: Terminator, The (1984), genres: Action|Sci-Fi|Thriller \n",
            "title: Blade Runner (1982), genres: Action|Sci-Fi|Thriller \n",
            "title: Clockwork Orange, A (1971), genres: Crime|Drama|Sci-Fi|Thriller \n",
            "title: Léon: The Professional (a.k.a. The Professional) (Léon) (1994), genres: Action|Crime|Drama|Thriller \n",
            "\n",
            "Here are some suggested movies for user 95\n",
            "title: Shawshank Redemption, The (1994), genres: Crime|Drama \n",
            "title: Forrest Gump (1994), genres: Comedy|Drama|Romance|War \n",
            "title: Pulp Fiction (1994), genres: Comedy|Crime|Drama|Thriller \n",
            "title: Silence of the Lambs, The (1991), genres: Crime|Horror|Thriller \n",
            "title: Fight Club (1999), genres: Action|Crime|Drama|Thriller \n",
            "title: Usual Suspects, The (1995), genres: Crime|Mystery|Thriller \n",
            "title: Schindler's List (1993), genres: Drama|War \n",
            "title: Braveheart (1995), genres: Action|Drama|War \n",
            "title: Godfather, The (1972), genres: Crime|Drama \n",
            "title: Toy Story (1995), genres: Adventure|Animation|Children|Comedy|Fantasy \n",
            "title: Fugitive, The (1993), genres: Thriller \n",
            "title: Lord of the Rings: The Fellowship of the Ring, The (2001), genres: Adventure|Fantasy \n",
            "title: Jurassic Park (1993), genres: Action|Adventure|Sci-Fi|Thriller \n",
            "title: American Beauty (1999), genres: Drama|Romance \n",
            "title: Apollo 13 (1995), genres: Adventure|Drama|IMAX \n",
            "title: Seven (a.k.a. Se7en) (1995), genres: Mystery|Thriller \n",
            "title: Lord of the Rings: The Return of the King, The (2003), genres: Action|Adventure|Drama|Fantasy \n",
            "title: Aladdin (1992), genres: Adventure|Animation|Children|Comedy|Musical \n",
            "title: Sixth Sense, The (1999), genres: Drama|Horror|Mystery \n",
            "title: Lion King, The (1994), genres: Adventure|Animation|Children|Drama|Musical|IMAX \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "make_predictions(user_id=580, num_recs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0XYul6ZdhK1",
        "outputId": "01171a7a-ac84-4260-f0bf-afa31d15e4cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are some movies that user 580 rated highly\n",
            "title: Shawshank Redemption, The (1994), genres: Crime|Drama \n",
            "title: Forrest Gump (1994), genres: Comedy|Drama|Romance|War \n",
            "title: Pulp Fiction (1994), genres: Comedy|Crime|Drama|Thriller \n",
            "title: Matrix, The (1999), genres: Action|Sci-Fi|Thriller \n",
            "title: Silence of the Lambs, The (1991), genres: Crime|Horror|Thriller \n",
            "title: Star Wars: Episode IV - A New Hope (1977), genres: Action|Adventure|Sci-Fi \n",
            "title: Fight Club (1999), genres: Action|Crime|Drama|Thriller \n",
            "title: Braveheart (1995), genres: Action|Drama|War \n",
            "title: Godfather, The (1972), genres: Crime|Drama \n",
            "title: Star Wars: Episode V - The Empire Strikes Back (1980), genres: Action|Adventure|Sci-Fi \n",
            "title: Usual Suspects, The (1995), genres: Crime|Mystery|Thriller \n",
            "title: Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981), genres: Action|Adventure \n",
            "title: American Beauty (1999), genres: Drama|Romance \n",
            "title: Star Wars: Episode VI - Return of the Jedi (1983), genres: Action|Adventure|Sci-Fi \n",
            "title: Terminator 2: Judgment Day (1991), genres: Action|Sci-Fi \n",
            "title: Saving Private Ryan (1998), genres: Action|Drama|War \n",
            "title: Fargo (1996), genres: Comedy|Crime|Drama|Thriller \n",
            "title: Lord of the Rings: The Fellowship of the Ring, The (2001), genres: Adventure|Fantasy \n",
            "title: Jurassic Park (1993), genres: Action|Adventure|Sci-Fi|Thriller \n",
            "title: Lord of the Rings: The Return of the King, The (2003), genres: Action|Adventure|Drama|Fantasy \n",
            "\n",
            "Here are some suggested movies for user 580\n",
            "title: Schindler's List (1993), genres: Drama|War \n",
            "title: Fugitive, The (1993), genres: Thriller \n",
            "title: Toy Story (1995), genres: Adventure|Animation|Children|Comedy|Fantasy \n",
            "title: Apollo 13 (1995), genres: Adventure|Drama|IMAX \n",
            "title: Back to the Future (1985), genres: Adventure|Comedy|Sci-Fi \n",
            "title: Lion King, The (1994), genres: Adventure|Animation|Children|Drama|Musical|IMAX \n",
            "title: Aladdin (1992), genres: Adventure|Animation|Children|Comedy|Musical \n",
            "title: Princess Bride, The (1987), genres: Action|Adventure|Comedy|Fantasy|Romance \n",
            "title: Groundhog Day (1993), genres: Comedy|Fantasy|Romance \n",
            "title: Inception (2010), genres: Action|Crime|Drama|Mystery|Sci-Fi|Thriller|IMAX \n",
            "title: Terminator, The (1984), genres: Action|Sci-Fi|Thriller \n",
            "title: Dark Knight, The (2008), genres: Action|Crime|Drama|IMAX \n",
            "title: Green Mile, The (1999), genres: Crime|Drama \n",
            "title: Independence Day (a.k.a. ID4) (1996), genres: Action|Adventure|Sci-Fi|Thriller \n",
            "title: Monsters, Inc. (2001), genres: Adventure|Animation|Children|Comedy|Fantasy \n",
            "title: True Lies (1994), genres: Action|Adventure|Comedy|Romance|Thriller \n",
            "title: Speed (1994), genres: Action|Romance|Thriller \n",
            "title: Breakfast Club, The (1985), genres: Comedy|Drama \n",
            "title: Mission: Impossible (1996), genres: Action|Adventure|Mystery|Thriller \n",
            "title: Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1964), genres: Comedy|War \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Out of curiosity, let's see if LightGCN did a better job than simply recommending all users the top 20 rated movies of all time. We have about 600 users, so I will define the top 20 movies as the ones with the highest average rating that have at least 30 ratings (5% of our user population)."
      ],
      "metadata": {
        "id": "_yGR1YX4MGiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get df of the top 20 rated movies on average\n",
        "rating_df = pd.read_csv(rating_path)\n",
        "size_df = rating_df.groupby('movieId').size().to_frame('size')\n",
        "rating_df = rating_df.groupby('movieId').agg('mean').join(other=size_df)\n",
        "rating_df = rating_df[rating_df['size'] >= 30].sort_values(by='rating', ascending=False)\n",
        "rating_df['movieId'] = rating_df.index\n",
        "rating_df = rating_df.head(20)\n",
        "top_movie_id_list = rating_df['movieId'].tolist()"
      ],
      "metadata": {
        "id": "WKkGqMzVMF8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get all unique users in evaluated split\n",
        "users = edge_index[0].unique()\n",
        "\n",
        "test_user_pos_items = get_user_positive_items(edge_index)\n",
        "\n",
        "# convert test user pos items dictionary into a list\n",
        "test_user_pos_items_list = [\n",
        "    test_user_pos_items[user.item()] for user in users]\n",
        "\n",
        "# determine the correctness of top20 predictions\n",
        "r = []\n",
        "for user in users:\n",
        "    ground_truth_items = test_user_pos_items[user.item()]\n",
        "    label = list(map(lambda x: x in ground_truth_items, top_movie_id_list))\n",
        "    r.append(label)\n",
        "r = torch.Tensor(np.array(r).astype('float'))\n",
        "\n",
        "recall, precision = RecallPrecision_ATk(test_user_pos_items_list, r, 20)\n",
        "ndcg = NDCGatK_r(test_user_pos_items_list, r, 20)\n",
        "\n",
        "print(f'Recall: {recall}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'NDCG: {ndcg}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NjJ5CokXQSG",
        "outputId": "400a62cd-ad6e-42e4-be00-eae3a73cc9cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall: 0.004841301124542952\n",
            "Precision: 0.017323480919003487\n",
            "NDCG: 0.01571757160127163\n"
          ]
        }
      ]
    }
  ]
}